<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Christopher A. Choquette-Choo</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-155314717-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-155314717-1');
  </script>

  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="author" content="Christopher A. Choquette-Choo">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <!-- Bootstrap core CSS -->
  <link href="css/bootstrap/bootstrap.min.css" rel="stylesheet">

  <link rel="stylesheet" type="text/css" href="css/academicons.min.css">
  <link rel="stylesheet" type="text/css" href="css/stylesheet.css">
  <link rel="stylesheet" type="text/css" href="css/styleoverrides.css">
  <script src="javascript.js"></script>

  <!-- <link rel="icon" type="image/png" href="images/seal_icon.png"> -->
  <script src="https://kit.fontawesome.com/c700ff3e4e.js" crossorigin="anonymous"></script>
  <!-- <script src="//code.jquery.com/jquery-1.12.4.min.js"></script> -->
</head>

<body>
<main role="main">

     <div class="jumbotron" style="padding-top: 1.5rem">
        <div class="container">
          <h1 class="media-heading" style="text-align: center">Christopher A. Choquette-Choo</h1>
          <div class="row">
            <div class="col-md-4">
              <img src="images/ChristopherChoquette.jpg" class="rounded img-fluid" alt="Christopher A. Choquette-Choo">
            </div>
            <div class="col-md-8">
                <span class="contacticon center">
                  <a href="https://scholar.google.com/citations?view_op=list_works&hl=en&user=oDE4I64AAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
                  <a href="data/2023-08-05-christopher choquette-stripped.pdf" target="_blank" title="CV"><i class="ai ai-cv"></i></a>
                  <a href="http://www.github.com/cchoquette" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
                  <a href="http://www.linkedin.com/in/christopher-choquette-choo/" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
                  <a href="https://twitter.com/Chris_Choquette" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>
                  <a href="bio.txt" target="_blank" title="Bio"><i class="fa fa-info-circle" aria-hidden="true"></i></a>
                </span>

                <p>I am a Research Scientist in Google Deepmind, on the DeepMind Privacy and Security team working on fundamental and applied research. My work has directly enabled several SOTA LLMs to be released, including Gemini (+1.5 Pro/Flash), Gemma (+CodeGemma), PaLM 2, and GBoard.
                <hr/>
                <!--              <p>My research interests are at the intersection of security, privacy, and machine learning. If you would like to learn more about my research, I recommend reading the blog posts I co-authored on <a href="https://www.cleverhans.io/" target="_blank">cleverhans.io</a>, for example about <a href="http://www.cleverhans.io/2020/07/20/unlearning.html" target="_blank">machine unlearning</a>, <a href="http://www.cleverhans.io/privacy/2018/04/29/privacy-and-machine-learning.html" target="_blank">differentially private ML</a>, or <a href="http://www.cleverhans.io/security/privacy/ml/2017/02/15/why-attacking-machine-learning-is-easier-than-defending-it.html" target="_blank">adversarial examples</a>.</p>-->
                <p>Previously, I was an AI Resident at Google and a researcher in the <a href="https://www.papernot.fr/">CleverHans Lab</a> at the Vector Institute. I graduated from the University of Toronto, where I had a <b><a href="https://www.schulichleaders.com/christopher-choquette-choo">full scholarship</a></b>.</p>
              <p class="mt-3 mb-0"><strong>Email:</strong> choquette[dot]christopher[at]gmail[dot]com</p>
<!--                <a href="/cdn-cgi/l/email-protection#5d33343e32313c2e732d3c2d382f3332291d2829322f32332932733e3c"><span class="__cf_email__" data-cfemail="4c22252f23202d3f623c2d3c293e2223380c3938233e23223823622f2d">[email&#160;protected]</span></a>-->
<!--              <p>-->
<!--                <a class="btn btn-info" href="https://papernot.fr/papernot_cv.pdf" target="_blank" role="button">CV &raquo;</a>&nbsp;-->
<!--                <a class="btn btn-success" href="http://www.cleverhans.io" target="_blank" role="button">Blog &raquo;</a>&nbsp;-->
<!--                <a class="btn btn-primary" href="https://www.twitter.com/nicolaspapernot" target="_blank" role="button">Twitter &raquo;</a>&nbsp;-->
<!--                <a class="btn btn-dark" href="https://scholar.google.com/citations?user=cGxq0cMAAAAJ&hl=en" target="_blank" role="button">Google Scholar &raquo;</a></p>-->
            </div>
          </div>

        </div>
      </div>
    <div class="jumbotron" style="padding-top: 1.5rem; background:white">
    <div class="container" style="background:white">

       <h4>Research</h4>

        <p>My focuses are privacy-preserving and adversarial machine learning. I mainly work on studying memorization and harms in language modelling as well as improving DP algorithms for machine learning; but,
           I've also worked on privacy auditing techniques, collaborative learning approaches, and methods for ownership-verification.</p> See my <a href="https://scholar.google.com/citations?view_op=list_works&hl=en&user=oDE4I64AAAAJ" target="_blank" title="Google Scholar">google scholar</a> for an up-to-date list.</p>

        <table class="table" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
<!--            <td style="display:block;margin:0px">-->
<!--              <div class="one">-->
<!--                <div class="two" id='nightsight_image'><img src='images/matfact.png' width="90%"></div></div>-->
<!--              </td>-->
            <td colspan="2" style="padding:20px;width:100%;vertical-align:middle">
                <h5>Technical Reports and Production Deployments</h5>

                <a href="https://storage.googleapis.com/deepmind-media/gemma/codegemma_report.pdf">
                    <papertitle>CodeGemma: Open Code Models Based on Gemma</papertitle></a><span class="badge badge-secondary">preprint</span><br><br>
                <a href="https://arxiv.org/abs/2403.08295">
                    <papertitle>Gemma: Open Models Based on Gemini Research and Technology</papertitle></a><span class="badge badge-secondary">preprint</span><br><br>
                <a href="https://arxiv.org/abs/2403.05530">
                    <papertitle>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</papertitle></a><span class="badge badge-secondary">preprint</span><br><br>
                <a href="https://arxiv.org/abs/2312.11805">
                    <papertitle>Gemini: A Family of Highly Capable Multimodal Models</papertitle></a><span class="badge badge-secondary">preprint</span><br><br>
                <a href="https://arxiv.org/abs/2305.10403">
                    <papertitle>PaLM 2 Technical Report</papertitle></a>: Led memorization analysis. <span class="badge badge-secondary">preprint</span><br><br>
                <a href="https://arxiv.org/abs/2305.18465"> <papertitle>Federated Learning of Gboard Language Models with Differential Privacy</papertitle></a>. Association for Computational Linguistics 2023. <span class="badge badge-primary">Conference</span>
              </td>
            </tr>
        <tr>
            <td colspan="2" style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2311.17035">
                    <papertitle>Scalable Extraction of Training Data from (Production) Language Models</papertitle></a>
                <br>
                Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. Feder Cooper, Daphne Ippolito, <b>Christopher A. Choquette-Choo</b>, Eric Wallace, Florian Tramèr, Katherine Lee
                    <span class="badge badge-secondary">preprint</span>
                <p></p>
                <p><b>We show that production aligned models can be provide a false-sense of privacy.</b>We show a vulnerability in Chat-GPT that lets us extract a lot of data. We scalably test for extractable memorization in many open-source models.</p>
              </td>
            </tr>
        <tr>
            <td colspan="2" style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2310.15526">
                    <papertitle>Privacy Amplification for Matrix Mechanisms</papertitle></a>
                <br>
                <b>Christopher A. Choquette-Choo</b>, Arun Ganesh, Thomas Steinke, Abhradeep Guha Thakurta
                <br>
                    <em>International Conference on Learning Representations (ICLR)</em>, 2024<span class="badge badge-danger">conference (+spotlight)</span>
                <p></p>
                <p><b>We prove a generic privacy amplification technique for DP-FTRL.</b></p>
              </td>
            </tr>
        <tr>
            <td colspan="2" style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2310.06771">
                    <papertitle>Correlated Noise Provably Beats Independent Noise for Differentially Private Learning</papertitle></a>
                <br>
                <b>Christopher A. Choquette-Choo</b>, Krishnamurthy Dj Dvijotham, Krishna Pillutla, Arun Ganesh, Thomas Steinke, Abhradeep Guha Thakurta
                <br>
                    <em>International Conference on Learning Representations (ICLR)</em>, 2024<span class="badge badge-primary">conference</span>
                <br>
                <em>International Workshop on Federated Learning in the Age of Foundation Models (FL@FM-NeurIPS)</em>2023</a><span class="badge badge-secondary">workshop</span>
                <p></p>
                <p><b>We prove bounds on DP-FTRL that show when it is better than DP-SGD.</b> We design new algorithms that are more efficient and as performant.</p>
              </td>
            </tr>
        <tr>
            <td colspan="2" style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://openreview.net/forum?id=qo21ZlfNu6&noteId=t0Fm6qcsZk">
                    <papertitle>Teach LLMs to Phish: Stealing Private Information from Language Models</papertitle></a>
                <br>
                Ashwinee Panda, <b>Christopher A. Choquette-Choo</b>, Zhengming Zhang, Yaoqing Yang, Prateek Mittal
                <br>
                    <em>International Conference on Learning Representations (ICLR)</em>, 2024<span class="badge badge-primary">conference</span>
                <p></p>
                <p><b>We show new attacks that can extract sensitive PII from LLMs with high success</b> and minimal assumptions.</p>
              </td>
            </tr>
        <tr>
            <td colspan="2" style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2310.09266">
                    <papertitle>User Inference Attacks on Large Language Models</papertitle></a>
                <br>
                Nikhil Kandpal, Krishna Pillutla, Alina Oprea, Peter Kairouz, <b>Christopher A. Choquette-Choo</b>, Zheng Xu
                <br>
                    <em>Socially Responsible Language Modelling Research (SoLaR)</em>, 2023<span class="badge badge-secondary">workshop</span>
                <br>
                    <em>International Workshop on Federated Learning in the Age of Foundation Models in Conjunction with NeurIPS (FL@FM-NeurIPS)</em>, 2023<span class="badge badge-secondary">workshop</span>
                <p></p>
                <p><b>We show new attacks against user's for LLMs tuned on sensitive user data that achieve high success.</b> We call these user inference attacks.</p>
              </td>
            </tr>
        <tr>
            <td colspan="2" style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2309.05610">
                    <papertitle>Privacy Side-Channels in Machine Learning</papertitle></a>
                <br>
                Edoardo Debenedetti, Giorgio Severi, Milad Nasr, <b>Christopher A. Choquette-Choo</b>, Matthew Jagielski, Eric Wallace, Nicholas Carlini, Florian Tramèr
                <br>
                  <em>Proceedings of 33th USENIX Security</em>, 2024<span class="badge badge-primary">conference</span>
                <p></p>
                <p><b>We show that an entire system---not just an ML model---need to be considered to ensure privacy.</b> We introduce new attacks against a variety of systems components that can invalidate guarantees on just an ML model.</p>
              </td>
            </tr>
        <tr>
            <td colspan="2" style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2311.06477">
                    <papertitle>Report of the 1st Workshop on Generative AI and Law</papertitle></a>
                <br>
                A. Feder Cooper*, Katherine Lee*, James Grimmelmann, Daphne Ippolito, Christopher Callison-Burch, <b>Christopher A. Choquette-Choo</b>, ...
                <br>
                    <span class="badge badge-secondary">preprint</span>
                <p></p>
                <p><b>We discuss and provide insights/opinions on the intersection of generative AI and law.</b></p>
              </td>
            </tr>

        <tr>
            <td colspan="2" style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2309.04662">
                    <papertitle>MADLAD-400: Multilingual And Document-Level Large Audited Dataset</papertitle></a>
                <br>
                Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, <b>Christopher A. Choquette-Choo</b>, Katherine Lee, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, Orhan Firat
                <br>
                    <em>Thirty-seventh Conference on Neural Information Processing Systems (Neurips)</em>, 2023<span class="badge badge-primary">conference</span>
                <p></p>
                <p><b>We introduce a new multilingual dataset, improve multilingual models, and show that translation tasks can be vulnerable to new privacy attacks.</b></p>
              </td>
            </tr>

        <tr>
            <td style="display:block;margin:0px">
              <div class="one">
                <div class="two" id='nightsight_image'><img src='images/matfact.png' width="90%"></div></div>
              </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2211.06530">
                    <papertitle>Multi-Epoch Matrix Factorization Mechanisms for Private Machine Learning</papertitle></a> <a href="https://github.com/google-research/federated/tree/master/multi_epoch_dp_matrix_factorization" target="_blank" title="GitHub" style='float: right'><i class="fab fa-github"></i></a>
                <br>
                <b>Christopher A. Choquette-Choo</b>, H. Brendan McMahan, Keith Rush, Abhradeep Thakurta
                <br>
                <em>40th International Conference on Machine Learning (ICML)</em>, 2023<span class="badge badge-danger">conference (+oral)</span>
                <p></p>
                <p><b>We achieve new state-of-the-art privacy-utility tradeoffs in DP ML</b>. We outperform DP-SGD without any need for privacy amplification.</p>
              </td>
            </tr>
        <tr>
            <td style="display:block;margin:0px">
              <div class="one">
                <div class="two" id='nightsight_image'><img src='images/autotune_compression.png' width="90%"></div></div>
              </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2307.10999">
                    <papertitle>Private Federated Learning with Autotuned Compression</papertitle></a> <a href="https://github.com/google-research/federated/tree/master/private_adaptive_linear_compression" target="_blank" title="GitHub" style='float: right'><i class="fab fa-github"></i></a>
                <br>
                Enayat Ullah*, <b>Christopher A. Choquette-Choo*</b>, Peter Kairouz*, Sewoong Oh*
                <br>
                <em>40th International Conference on Machine Learning (ICML)</em>, 2023<span class="badge badge-primary">conference</span>
                <p></p>
                <p><b>We show how to achieve impressive compression rates in the online setting, i.e., without prior tuning</b>. Our best algorithms can achieve close to the best-known compression rates.</p>
              </td>
            </tr>
        <tr>
        <tr>
            <td style="display:block;margin:0px">
              <div class="one">
                <div class="two" id='nightsight_image'><img src='images/amplified.png' width="90%"></div></div>
              </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2306.08153">
                    <papertitle>(Amplified) Banded Matrix Factorization: A unified approach to private training</papertitle></a>
                <br>
                <b>Christopher A. Choquette-Choo</b>, Arun Ganesh, Ryan McKenna, H. Brendan McMahan, Keith Rush, Abhradeep Guha Thakurta, Zheng Xu
                <br>
                <em>Thirty-seventh Conference on Neural Information Processing Systems (Neurips)</em>, 2023<span class="badge badge-primary">conference</span>
                <p></p>
                <p><b>We show that DP-FTRL is strictly better than DP-SGD.</b> In doing so, we get new state-of-the-art.</p>
              </td>
            </tr>
        <tr>
            <td colspan="2" style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2306.15447">
                    <papertitle>Are aligned neural networks adversarially aligned?</papertitle></a>
                <br>
                Nicholas Carlini, Milad Nasr, <b>Christopher A. Choquette-Choo</b>, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tramèr, Ludwig Schmidt
                <br>
                     <em>Thirty-seventh Conference on Neural Information Processing Systems (Neurips)</em>, 2023<span class="badge badge-primary">conference</span>
                <p></p>
                <p><b>We introduce attacks against multi-modal models that subvert alignment techniques.</b> In doing so, we show that current NLP attacks are not sufficiently powerful to evaluate adversarial alignment.</p>
              </td>
            </tr>
        <tr>
            <td colspan="2" style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2302.10149">
                    <papertitle>Poisoning Web-Scale Training Datasets is Practical</papertitle></a>
                <br>
                Nicholas Carlini, Matthew Jagielski, <b>Christopher A. Choquette-Choo</b>, Daniel Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, Florian Tramèr
                <br>
                    <em>IEEE Symposium on Security and Privacy (IEEE S&P)</em>, 2024<span class="badge badge-primary">conference</span>
                <p></p>
                <p><b>We show that modern large-scale datasets can be cost-effectively poisoned</b>. </p>
              </td>
            </tr>
        <tr>
            <td style="display:block;margin:0px">
              <div class="one">
                <div class="two" id='nightsight_image'><img src='images/distillation_mi.png' width="75%"></div></div>
              </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2303.03446">
                    <papertitle>Students Parrot Their Teachers: Membership Inference on Model Distillation</papertitle></a>
                <br>
                    Matthew Jagielski, Milad Nasr, Katherine Lee, <b>Christopher A. Choquette-Choo</b>, Nicholas Carlini
                <br>
                    <span class="badge badge-danger">conference (+oral)</span>
                <p></p>
                <p><b>We show that, without formal guarantees, distillation provides limited privacy benefits</b>. We also show the first attack where a model can leak information about private data without ever having been queried on that private data.</p>
              </td>
            </tr>
        <td style="display:block;margin:0px">
              <div class="one">
                <div class="two" id='nightsight_image'><img src='images/preventing_verbatim.png' width="100%"></div></div>
              </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2210.17546">
                    <papertitle>Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy</papertitle></a>
                <br>
                Daphne Ippolito, Florian Tramèr*, Milad Nasr*, Chiyuan Zhang*, Matthew Jagielski*, Katherine Lee*, <b>Christopher A. Choquette-Choo</b>*, Nicholas Carlini
                <br>
                    <em>* Equal contribution. The names are ordered randomly.</em>
                    <em>Proceedings of the 15th International Natural Language Generation Conference (INLG)</em> 2023<span class="badge badge-danger">conference (best-paper runner-up)</span>
                <p></p>
                <p><b>We show that models can paraphrase memorizations</b>. Thus, models can evade filters designed specifically to prevent verbatim memorization, like those implemented for CoPilot.</p>
              </td>
            </tr>
        <tr>
            <td colspan="2" style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2402.09403">
                    <papertitle>Auditing Private Prediction</papertitle></a>
                <br>
                Karan Chadha, Matthew Jagielski, Nicolas Papernot, <b>Christopher A. Choquette-Choo</b>, Milad Nasr
                    <em>International Conference on Machine Learning (ICML)</em>, 2024 <span class="badge badge-primary">conference</span>
                <p></p>
              </td>
            </tr>
        <tr>
            <td colspan="2" style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2208.03567">
                    <papertitle>Proof-of-Learning is Currently more Broken than you Think</papertitle></a>
                <br>
                Congyu Fang*, Hengrui Jia*, Anvith Thudi, Mohammad Yaghini, <b>Christopher A. Choquette-Choo</b>, Natalie Dullerud, Varun Chandrasekaran, Nicolas Papernot
                <br>
                    <em>* Equal contribution.</em>
            <em>Proceedings of the 8th IEEE Euro S&P 2023</em><span class="badge badge-primary">conference</span>
                <p></p>
                <p><b>We break current Proof-of-Learning schemes</b>. Though we design schemes that are robust to all current attacks, we show that it is an open problem to provide formal guarantees.</p>
              </td>
            </tr>
        <tr>
            <td colspan="2" style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2310.16678">
                    <papertitle>Robust and Actively Secure Serverless Collaborative Learning</papertitle></a>
                <br>
                Nicholas Franzese, Adam Dziedzic, <b>Christopher A. Choquette-Choo</b>, Mark R. Thomas, Muhammad Ahmad Kaleem, Stephan Rabanser, Congyu Fang, Somesh Jha, Nicolas Papernot, Xiao Wang
                <br>
                <em>Thirty-seventh Conference on Neural Information Processing Systems (Neurips)</em> 2023<span class="badge badge-primary">conference</span>
                <p></p>
                <p><b></b></p>
              </td>
            </tr>
        <tr>
            <td colspan="2" style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2211.15410">
                    <papertitle>Private Multi-Winner Voting for Machine Learning</papertitle></a>
                <br>
                Adam Dziedzic, <b>Christopher A. Choquette-Choo</b>, Natalie Dullerud, Vinith Menon Suriyakumar, Ali Shahin Shamsabadi, Muhammad Ahmad Kaleem, Somesh Jha, Nicolas Papernot, Xiao Wang
                <br>
                <em>Proceedings on 23rd Privacy Enhancing Technologies Symposium (PETS 2023)</em><span class="badge badge-primary">conference</span>
                <p></p>
                <p><b></b></p>
              </td>
            </tr>
        <tr>
            <td colspan="2" style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2210.02156">
                    <papertitle>Fine-tuning with differential privacy necessitates an additional hyperparameter search</papertitle></a>
                <br>
                Yannis Cattan, <b>Christopher A Choquette-Choo</b>, Nicolas Papernot, Abhradeep Thakurta
                <br>
                  <span class="badge badge-secondary">preprint</span>
                <p></p>
                <p><b>Do proper hyper-parameter tuning with DP.</b> Check how many layers you should tune.</p>
              </td>
            </tr>
        <tr>
            <td style="display:block;margin:0px">
              <div class="one">
                <div class="two" id='nightsight_image'><img src='images/cost_secagg.png' width="100%"></div></div>
              </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2203.03761.pdf">
                  <papertitle>The Fundamental Price of Secure Aggregation in Differentially Private Federated Learning</papertitle> <a href="https://github.com/google-research/federated/tree/master/private_linear_compression" target="_blank" title="GitHub" style='float: right'><i class="fab fa-github"></i></a>
                </a>
                <br>
                Wei-Ning Chen*, <b>Christopher A. Choquette-Choo*</b>, Peter Kairouz*, Ananda Theertha Suresh*
                <br>
                <em>* Equal contribution. The names are ordered alphabetically.</em>
                <br>
                <a href="https://openreview.net/forum?id=NqikEZ6ABea">
                    <em>International Conference on Machine Learning (ICML)</em>, 2022 </a><span class="badge badge-danger">conference (+spotlight)</span>
                <br>
                <em>Theory and Practice of Differential Privacy (TPDP) Workshop</em>, 2022 </a><span class="badge badge-secondary">workshop</span>
                <br>

                <br>
                <p></p>
                <p><b>We characterize the fundamental communication costs of Federated Learning (FL) under Secure Aggregation (SecAgg) and Differential Privacy (DP)</b>, two privacy-preserving mechanisms that are commonly used with FL. We prove our optimality for worst-case settings which provides significant improvements over prior work, and show that improvements can be made under additional assumptions, e.g., data sparisty. Extensive empirical evaluations support our claims, showing costs as low as 1.2 bits per parameter on Stack Overflow with <em><4%</em> relative decrease in test-time model accuracy.</p>
              </td>
            </tr>
        <tr>
            <td style="display:block;margin:0px">
              <div class="one">
                <div class="two" id='nightsight_image'><img src='images/cost_comm_efficient.png' width="100%"></div></div>
              </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://openreview.net/forum?id=NqikEZ6ABea">
                  <papertitle>Communication Efficient Federated Learning with Secure Aggregation and Differential Privacy</papertitle>
                </a>
                <br>
                Wei-Ning Chen*, <b>Christopher A. Choquette-Choo*</b>, Peter Kairouz*
                <br>
                <a href="https://openreview.net/forum?id=NqikEZ6ABea">
                <em>Privacy in Machine Learning Workshop at Neurips</em>, 2021 </a><span class="badge badge-secondary">workshop</span>
                <br>
                <em>* Equal contribution. The names are ordered alphabetically.</em>
                <br>
                <p></p>
                <p>We show that in the worst-case, differentially-private federated learning with secure aggregation requires <em>Ω(d)</em> bits. Despite this, we discuss how to leverage near-sparsity to compress updates by more than <em>50x</em> with modest noise multipliers of <em>0.4</em> by using sketching.</p>
              </td>
            </tr>
        <tr>
            <td colspan="2" style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2103.05633">
                  <papertitle>Proof-of-Learning: Definitions and Practice</papertitle> <a href="https://github.com/cleverhans-lab/Proof-of-Learning" target="_blank" title="GitHub" style='float: right'><i class="fab fa-github"></i></a>
                </a>
                <br>
              Hengrui Jia^, Mohammad Yaghini^, <b>Christopher A. Choquette-Choo*</b>, Natalie Dullerud*, Anvith Thudi*, Varun Chandrasekaran, Nicolas Papernot
                <br>
              <em>Proceedings of the 42nd IEEE Symposium on Security and Privacy</em>, San Francisco, CA, 2021 <span class="badge badge-primary">conference</span>
                <br>
                <em>^,* Equal contribution. The names are ordered alphabetically.</em>
                <br>
                <p></p>
                <p><b>How can we prove that a machine learning model owner trained their model?</b> We define the problem of Proof-of-Learning (PoL) in machine learning and provide a method for it that is robust to several spoofing attacks. This protocol enables model ownership verification and robustness against byzantines workers (in a distributed learning setting).</p>
              </td>
            </tr>
        <tr>
            <td style="display:block;margin:0pxe">
              <div class="one">
                <div class="two" id='nightsight_image'><img src='images/capc.png' width="100%"></div></div>
              </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2102.05188">
                  <papertitle>CaPC Learning: Confidential and Private Collaborative Learning</papertitle> <a href="https://github.com/cleverhans-lab/capc-iclr" target="_blank" title="GitHub" style='float: right'><i class="fab fa-github"></i></a>
                </a>
                <br>
                <b>Christopher A. Choquette-Choo*</b>, Natalie Dullerud*, Adam Dziedzic*, Yunxiang Zhang*, Somesh Jha, Nicolas Papernot, Xiao Wang
                <br>
              <em>9th International Conference on Learning Representations (ICLR)</em>, 2021 <span class="badge badge-primary">conference</span>
                <br>
                <em>* Equal contribution. The names are ordered alphabetically.</em>
                <br>
                <p></p>
                <p><b>We design a protocol for collaborative learning that ensures both the privacy of the training data and confidentiality of the test data in a collaborative setup.</b> Our protocol can provide several percentage-points improvement to models, especially on subpopulations that the model underperforms on, with a modest privacy budget usage of less than 20. Unlike prior work, we enable collaborative learning of heterogeneous models amongst participants. Unlike differentially private federated learning, which requires ~1 million participants, our protocol can work in regimes of ~100 participants. </p>
              </td>
            </tr>
          <tr>
            <td style="display:block;margin:0px">
              <div class="one">
                <div class="two" id='nightsight_image'><img src='images/defenseplot.png' width="100%"></div></div>
              </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2007.14321">
                  <papertitle>Label-Only Membership Inference Attacks</papertitle> <a href="https://github.com/label-only/membership-inference" target="_blank" title="GitHub" style='float: right'><i class="fab fa-github"></i></a>
                </a>
                <br>
                <b>Christopher A. Choquette-Choo</b>, Florian Tramèr, Nicholas Carlini, Nicolas Papernot
                <br>
                <em>38th International Conference on Machine Learning (ICML)</em>, 2021 <span class="badge badge-danger">conference (+spotlight)</span>
                <br>
                <p></p>
                <p><b>What defenses properly defend against all membership inference threats?</b> We expose and show that <em>confidence-masking</em> -- defensive obfuscation of confidence-vectors -- is not a viable defense to Membership Inference. We do this by introducing (3) label-only attacks, which bypass this defense and match typical confidence-vector attacks. In an extensive evaluation of defenses,including the first evaluation of data augmentations and transfer learning as defenses, we further show that Differential Privacy can defend against average- and worse-case Membership Inference attacks.</p>
              </td>
            </tr>
          <tr>
            <td style="display:block;margin:0px">
              <div class="one">
                <div class="two" id='nightsight_image'><img src='images/ewe.png' width="80%"></div></div>
              </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2002.12200">
                  <papertitle>Entangled Watermarks as a Defense against Model Extraction</papertitle> <a href="https://github.com/cleverhans-lab/entangled-watermark" target="_blank" title="GitHub" style='float: right'><i class="fab fa-github"></i></a>
                </a>
                <br>
                Hengrui Jia, <b>Christopher A. Choquette-Choo</b>, Varun Chandrasekaran, Nicolas Papernot
                <br>
                <em>Proceedings of 30th USENIX Security</em>, 2021 <span class="badge badge-primary">conference</span>
                <p></p>
                <p><b>How can we enable an IP owner to reliably claim ownership of a stolen model?</b> We explore entangling of watermarks to task data to ensure that stolen models learn these watermarks as well. Our improved watermarks enable IP owners to claim ownership with 95% confidence in less than 10 queries to the stolen model.</p>
              </td>
            </tr>

          <tr>
            <td style="display:block;margin:0px">
              <div class="one">
                <div class="two" id='nightsight_image'><img src='images/sisa.png' width="100%"></div></div>
              </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/1912.03817">
                  <papertitle>Machine Unlearning</papertitle> <a href="https://github.com/cleverhans-lab/machine-unlearning" target="_blank" title="GitHub" style='float: right'><i class="fab fa-github"></i></a>
                </a>
                <br>
                Lucas Bourtoule*, Varun Chandrasekaran*, <b>Christopher A. Choquette-Choo*</b>, Hengrui Jia*, Adelin Travers*, Baiwu Zhang*, David Lie, Nicolas Papernot
                <br>
                <em>Proceedings of the 42nd IEEE Symposium on Security and Privacy</em>, San Francisco, CA, 2021 <span class="badge badge-primary">conference</span>
                <br>
                <em>* Equal contribution. The names are ordered alphabetically.</em>
                <br>
                <p></p>
                <p><b>How can we enable efficient and guaranteed retraining of machine learning models?</b> We define requirements for machine unlearning and study a stricter unlearning requirement whereby unlearning a datapoint is guaranteed to be equivalent to as if we had never trained on it. To this end, we improve on the naive retraining-from-scratch approach to provide a better accuracy-efficieny tradeoff. We also study how a priori knowledge of the distribution of requests can further improve efficiency.</p>
              </td>
            </tr>

          <tr>
            <td style="display:block;margin:0px">
              <div class="one">
                <div class="two" id='nightsight_image'><img src='images/neuralnetwork.png' width="100%"></div></div>
             <!--  <script type="text/javascript">
                function nightsight_start() {
                  document.getElementById('nightsight_image').style.opacity = "1";
                }

                function nightsight_stop() {
                  document.getElementById('nightsight_image').style.opacity = "0";
                }
                nightsight_stop()
              </script -->
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1910.05835">
                <papertitle>A multi-label, dual-output deep neural network for automated bug triaging</papertitle>
              </a>
              <br>
              <b>Christopher A. Choquette-Choo</b>, David Sheldon, Jonny Proppe, John Alphonso-Gibbs, Harsha Gupta
              <br>
              <em>ICMLA</em>, 2019  &nbsp|&nbsp DOI: 10.1109/ICMLA.2019.00161 <span class="badge badge-primary">conference</span>
              <br>
              <p></p>
              <p><b>How can we triage bugs more effectively?</b> By utilizing a model's own knowledge of an analogous lower-dimensionality solution-space (predicting the correct team assignment), we can achieve higher accuracies on the higher-dimensionality solution-space (predicting the correct engineer assignment).</p>
            </td>
          </tr>


        </tbody></table>
        <hr/>

       <h4>Paper Talks</h4>
        <div class="row mb-3">
            <div class="col-sm-8">
                <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="100%" valign="center">
              <b>DP-Follow-The-Regularized-Leader: State-of-the-art Optimizers for Private Machine Learning.</b>
              <br>
              Invited talk at Institute of Science and Technology Austria (ISTA) for Prof. Christoph Lampert in 2024.
            </td>
          </tr>
          <tr>
            <td width="100%" valign="center">
              <b>DP-Follow-The-Regularized-Leader: State-of-the-art Optimizers for Private Machine Learning.</b>
              <br>
              Invited talk at the “Federated Learning on the Edge” AAAI Spring 2024 Symposium.
            </td>
          </tr>
          <tr>
            <td width="100%" valign="center">
              <b>Hosted the "Private Optimization with Correlated Noise" invited session. Co-presented first talk.</b>
              <br>
              Presented at Information Theory and Applications (ITA) 2024.
            </td>
          </tr>
          <tr>
            <td width="100%" valign="center">
              <b>Poisoning Web-Scale Training Datasets is Practical.</b>
              <br>
              Presented in lecture for Prof. Varun Chandrasekaran at University of Illinois at Urbana-Champaign in 2024.
            </td>
          </tr>
          <tr>
            <td width="100%" valign="center">
              <b>Multi-Epoch Matrix Factorization Mechanisms for Private Machine Learning.</b>
              <br>
              Presented at ICML 2023
              <br>
                <a href="https://icml.cc/virtual/2023/oral/25561">ICML Website</a> or <a href="https://slideslive.com/39006728">SlidesLive archive</a>
            </td>
          </tr>
          <tr>
            <td width="100%" valign="center">
              <b>The Fundamental Price of Secure Aggregation in Differentially Private Machine Learning</b>
              <br>
              Invited talk at the University of Toronto & Vector Institute for Prof. Nicolas Papernot.
              <br>
                 <a href="https://icml.cc/virtual/2022/spotlight/17530">ICML Website</a>
            </td>
          </tr>
          <tr>
            <td width="100%" valign="center">
              <b>The Fundamental Price of Secure Aggregation in Differentially Private Machine Learning</b>
              <br>
              Presented at ICML 2022
              <br>
                 <a href="https://icml.cc/virtual/2022/spotlight/17530">ICML Website</a>
            </td>
          </tr>
        <tr>
            <td width="100%" valign="center">
              <b>Label-Only Membership Inference Attacks</b>
              <br>
              Presented at ICML 2021
              <br>
                 <a href="https://icml.cc/virtual/2021/spotlight/9416">ICML Website</a>
            </td>
          </tr>
        <tr>
            <td width="100%" valign="center">
              <b>Proof-of-Learning Definitions and Practices</b>
              <br>
              Presented at IEEE S&P 2021
              <br>
                 <a href="https://www.youtube.com/watch?v=4h_76xTTPvk&ab_channel=IEEESymposiumonSecurityandPrivacy">S&P Youtube Channel</a>
            </td>
          </tr>
        <tr>
            <td width="100%" valign="center">
              <b>Machine Unlearning</b>
              <br>
              Presented at IEEE S&P 2021
              <br>
                 <a href="https://www.youtube.com/watch?v=xUnMkCB0Gns&ab_channel=IEEESymposiumonSecurityandPrivacy">S&P Youtube Channel</a>
            </td>
          </tr>
        </tbody></table>
            </div>
            </div>
       <h4>Invited Talks</h4>
        <div class="row mb-3">
            <div class="col-sm-8">
                <table width="100%" align="center" border="0" cellpadding="20"><tbody>
                  <tr>
                    <td style="padding:20px;width:20%;vertical-align:middle"><img src="images/privacy_production_ml.png" width="100%"></td>
                    <td width="70%" valign="center">
                      <b>Privacy Considerations of Production Machine Learning.</b>
                      <br>
                      Presented at Ml Ops World: New York Area Summit
                      <br>
                        Contact me for slides. Unfortunately, the video is not publicly available :(.
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:20px;width:20%;vertical-align:middle"><img src="images/ReWorkVideoSnippet.png" width="100%"></td>
                    <td width="70%" valign="center">
                      <b>Adversarial Machine Learning: Ensuring Security and Privacy of ML Models and Sensitive Data.</b>
                      <br>
                      Presented at the REWORK Responsible AI Summit</a> 2019.
                      <br>
                        Available as a part of the <a href="https://videos.re-work.co/playlists/22-privacy-security">Privacy and Security in Machine Learning</a> package.
                    </td>
                  </tr>
                 </tbody></table>
            </div>
<!--              <div align="center"><b>Adversarial Machine Learning: Ensuring Security and Privacy of ML Models and Sensitive Data.</b></div>-->
<!--                <img src="images/ReWorkVideoSnippet.png" width=100>-->
<!--                Presented at the REWORK Responsible AI Summit 2019 in Montreal.-->
<!--              <br>-->
<!--              Available as a part of the <a href="https://videos.re-work.co/playlists/22-privacy-security">Privacy and Security in Machine Learning</a> package.-->
<!--&lt;!&ndash;                <iframe width="560" height="315" src="http://videos.re-work.co/videos/1763-adversarial-machine-learning-ensuring-security-of-ml-models-and-sensitive-data-christopher-choquette-choo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>&ndash;&gt;-->
<!--            </div>-->
        </div>
        <h4>Professional Services</h4>
        <ul>
<!--            <li>PC for the GenAI and Law Workshop at Neurips 2023.</li>-->
            <li>AC for the 2024 Thirty-eighth Neural Information Processing Systems (Neurips).</li>
            <li>PC for the 2025 IEEE Security and Privacy (S&P) conference.</li>
            <li>Reviewer for the 2024 Forty-first International Conference on Machine Learning (ICML).</li>
            <li>Reviewer for the 2024 Twelfth International Conference on Learning Representations (ICLR).</li>
            <li>PC for the 2024 IEEE Security and Privacy (S&P) conference.</li>
            <li>PC for the 2023 Generative AI + Law (GenLaw)'23 Workshop at ICML</li>
            <li>Top Reviewer for the 2023 Neural Information Processing Systems (Neurips).</li>
            <li>Reviewer for 2023 International Conference on Machine Learning (ICML)</li>
            <li>Invited Reviewer for Nature Machine Intelligence Journal 2023</li>
            <li>Session Chair of DL: Robustness for the 2022 International Conference on Machine Learning (ICML).</li>
            <li>Reviewer for the 2022 Neural Information Processing Systems (Neurips).</li>
            <li>Invited Reviewer for Nature Machine Intelligence Journal 2022</li>
            <li>Outstanding Reviewer for 2022 International Conference on Machine Learning (ICML)</li>
            <li>Invited Reviewer for 2022 IEEE Transactions on Emerging Topics in Computing</li>
            <li>External Reviewer for 2022 USENIX Security Symposium</li>
            <li>External Reviewer for 2022 IEEE Symposium on Security and Privacy</li>
            <li>Reviewer for 2021 Journal of Machine Learning Research</li>
            <li>External Reviewer for 2021 International Conference on Machine Learning (ICML)</li>
            <li>External Reviewer for 2021 USENIX Security Symposium</li>
            <li>External Reviewer for 2021 IEEE Symposium on Security and Privacy</li>
            <li>Reviewer for the 2020 Machine Learning for the Developing World (ML4D) workshop at NeurIPS</li>
        </ul>
      </div> <!-- /container -->
    </div> <!-- /jumbotron -->
    </main>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script data-cfasync="false" src="/cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js"></script><script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="js/bootstrap.min.js"></script>
  </body>
<p style="text-align:right;font-size:small;">This website was based off <a href="https://jonbarron.info/">Jon Barron</a>'s.</p>
</html>
